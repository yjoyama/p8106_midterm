---
title: "Midterm Project"
author: "Yuki Joyama"
date: "2024-03-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)

library(tidyverse)
library(ggplot2)
library(rsample) 
library(caret)
library(tidymodels)
library(plotmo)
library(earth)
library(vip)
library(flextable)

# setup plot theme
theme_set(
  theme_bw() +
    theme(legend.position = "top")
  )
```

```{r dataprep}
# read RData file
df_recov <- get(load("./data/recovery.RData")) |> 
  janitor::clean_names()

head(df_recov)

# partition (training:test=80:20)
set.seed(2024)
data_split = initial_split(df_recov, prop = .80)
# training data
df_train = training(data_split) |> 
  select(!id)
# test data
df_test = testing(data_split) |> 
  select(!id)

# set up 10-fold CV
ctrl1 <- trainControl(
  method = "cv",
  number = 10
)
```

# Model training
I assume that the predictors of this dataset have a non-linear relationship with the response variable (COVID-19 recovery time) so I will train with GAM and MARS models as well as linear model, LASSO, Ridge, Elastic net regression, PLS and PCR. For the model training, I am going to implement 10-fold cross validation to find the optimal tuning parameters for each model. Finally, I will look at RMSE to select the final model. 

## Linear model
I will fit linear model for the recovery time and include all the other variables as predictors. 
```{r lm}
set.seed(2024)

lm.fit <- train(
  recovery_time ~ .,
  data = df_train,
  method = "lm",
  metric = "RMSE",
  trControl = ctrl1
)

# check the model
summary(lm.fit$finalModel)

# obtain the test error
lm.pred <- predict(lm.fit, newdata = df_test)
mean((lm.pred - pull(df_test, "recovery_time"))^2) 
```

Ten predictors showed statistically significant effect on the response variable. The test error is `r round(mean((lm.pred - pull(df_test, "recovery_time"))^2), 3) `.

## Lasso
```{r lasso}
set.seed(2024)

# find tuning parameter by CV
lasso.fit <- 
  train(
    recovery_time ~ .,
    data = df_train,
    method = "glmnet",
    tuneGrid = expand.grid(
      alpha = 1,
      lambda = exp(seq(2, -10, length = 100))
    ),
    trControl = ctrl1
  )

coef(lasso.fit$finalModel, s = lasso.fit$bestTune$lambda) 

# plot RMSE and lambda
plot(lasso.fit, xTrans = log)

# print the best tuning parameter
lasso.fit$bestTune

# obtain the test error
lasso.pred <- predict(lasso.fit, newdata = df_test |> select(!recovery_time))
mean((lasso.pred - pull(df_test, "recovery_time"))^2) 
```

16 predictors are included in the model (excet for SBP). The optimal tuning parameter is $\lambda=$ `r lasso.fit$bestTune`.  
The test error is `r round(mean((lasso.pred - pull(df_test, "recovery_time"))^2), 3) `.

## Ridge
```{r ridge}
set.seed(2024)

# find tuning parameter by CV
ridge.fit <- 
  train(
    recovery_time ~ .,
    data = df_train,
    method = "glmnet",
    tuneGrid = expand.grid(
      alpha = 0,
      lambda = exp(seq(2, -2, length = 100))
    ),
    trControl = ctrl1
  )

coef(ridge.fit$finalModel, s = ridge.fit$bestTune$lambda) 

# plot RMSE and lambda
plot(ridge.fit, xTrans = log)

# print the best tuning parameter
ridge.fit$bestTune

# obtain the test error
ridge.pred <- predict(ridge.fit, newdata = df_test |> select(!recovery_time))
mean((ridge.pred - pull(df_test, "recovery_time"))^2) 
```

All predictors are included in the model. The optimal tuning parameter is $\alpha=$ `r ridge.fit$bestTune`.  
The test error is `r round(mean((ridge.pred - pull(df_test, "recovery_time"))^2), 3) `.

## Elastic net
```{r enet}
set.seed(2024)

# find tuning parameter by CV
enet.fit <- 
  train(
    recovery_time ~ .,
    data = df_train,
    method = "glmnet",
    tuneGrid = expand.grid(
      alpha = seq(0, 1, length = 20),
      lambda = exp(seq(2, 0, length = 100))
    ),
    trControl = ctrl1
  )

# check the best tuning parameter
enet.fit$bestTune

# plot RMSE, lambda and alpha
myCol <- rainbow(25)
myPar <- list(
  superpose.symbol = list(col = myCol),
  superpose.line = list(col = myCol)
)

plot(enet.fit, par.settings = myPar, xTrans = log)

# coefficients in the final model
coef(enet.fit$finalModel, s = enet.fit$bestTune$lambda)

# obtain predicted values
enet.pred <- predict(enet.fit, newdata = df_test |> select(!recovery_time))
# test error
mean((enet.pred - pull(df_test, "recovery_time"))^2)
```

All predictors are included in the model. The optimal tuning parameter is $\alpha=$ `r enet.fit$bestTune[1]` and $\lambda=$ `r enet.fit$bestTune[2]`.  
The test error is `r round(mean((enet.pred - pull(df_test, "recovery_time"))^2), 3) `.

## Principal Components Regression model (PCR)
```{r pcr}
# prepare x and y
# training 
x <- model.matrix(recovery_time ~ ., data = df_train)[, -1]
y <- df_train$recovery_time

# test
x2 <- model.matrix(recovery_time ~ ., df_test)[, -1]
y2 <- df_test$recovery_time

set.seed(2024)

pcr.fit <- train(
  x, y,
  method = "pcr",
  tuneGrid = data.frame(ncomp = 1:17),
  trControl = ctrl1,
  preProcess = c("center", "scale")
)

summary(pcr.fit)

# obtain predicted values 
pred.pcr <- predict(
  pcr.fit, 
  newdata = x2
)

# visualize RMSE and the number of components
ggplot(pcr.fit, highlight = T) 

# test MSE
mean((pred.pcr - y2)^2)
```

The PCR model is composed of 17 components. 

The test error is `r round(mean((pred.pcr - pull(df_test, "recovery_time"))^2), 3) `.

## Partial Least Squares model (PLS)
```{r pls}
set.seed(2024)

pls.fit <- train(
  x, y,
  method = "pls",
  tuneGrid = data.frame(ncomp = 1:17),
  trControl = ctrl1,
  preProcess = c("center", "scale")
)

summary(pls.fit)

# obtain predicted values 
pred.pls <- predict(
  pls.fit, 
  newdata = x2
)

# visualize RMSE and the number of components
ggplot(pls.fit, highlight = T) 

# test MSE
mean((pred.pls - y2)^2)
```

The PCR model is composed of 12 components. 

The test error is `r round(mean((pred.pls - pull(df_test, "recovery_time"))^2), 3) `.

## MARS
```{r mars}
set.seed(2024)

# fit mars model
mars.fit <- train(
  x = df_train[1:14],
  y = df_train$recovery_time,
  method = "earth",
  tuneGrid = expand.grid(degree = 1:5, nprune = 2:30),
  metric = "RMSE",
  trControl = ctrl1
)

summary(mars.fit$finalModel)

# best tuning parameters
mars.fit$bestTune

# plot
plot(mars.fit)

# relative variable importance
vip(mars.fit$finalModel, type = "nsubsets")

# obtain the test error
mars.pred <- predict(mars.fit, newdata = df_test |> select(!recovery_time))
mean((mars.pred - pull(df_test, "recovery_time"))^2) 
```

The best tuning parameters selected from the cross validation is nprune (the upper bound of the number of terms) = `r mars.fit$bestTune[1]` and degree = `r mars.fit$bestTune[2]`.  

The final model can be expressed as the following:  
$\hat{y}$ = `r round(coef(mars.fit$finalModel)[1], 3)` + `r round(coef(mars.fit$finalModel)[2], 3)` $\times$ h(30.3 - bmi) + `r round(coef(mars.fit$finalModel)[3], 3)` $\times$ h(bmi - 30.3) * studyB + `r round(coef(mars.fit$finalModel)[4], 3)` $\times$ vaccine + `r round(coef(mars.fit$finalModel)[5], 3)` $\times$ h(164 - height) * h(bmi - 30.3) * studyB + `r round(coef(mars.fit$finalModel)[6], 3)` $\times$ h(bmi - 25.7) + `r round(coef(mars.fit$finalModel)[7], 3)` $\times$ h(87.6 - weight) * h(bmi - 30.3) * studyB  
where $h(.)$ is hinge function.  

The test error is `r round(mean((mars.pred - pull(df_test, "recovery_time"))^2), 3) `.

## GAM
```{r gam}
set.seed(2024)

# fit gam model
gam.fit <- train(
  x = df_train[1:14],
  y = df_train$recovery_time,
  method = "gam",
  metric = "RMSE",
  trControl = ctrl1
)

summary(gam.fit$finalModel)

# best tuning parameters
gam.fit$bestTune

# obtain the test error
gam.pred <- predict(gam.fit, newdata = df_test |> select(!recovery_time))
mean((gam.pred - pull(df_test, "recovery_time"))^2) 
```

The model includes 14 predictors. age, sbp, ldl, bmi, height, weight have non-linear terms in the model. 

The test error is `r round(mean((gam.pred - pull(df_test, "recovery_time"))^2), 3) `.

## Model Comparison
```{r}
# resampling
resamp <- resamples(list(
  lm = lm.fit,
  lasso = lasso.fit,
  ridge = ridge.fit,
  elastic_net = enet.fit,
  pcr = pcr.fit,
  pls = pls.fit,
  mars = mars.fit,
  gam = gam.fit
))

summary(resamp)

# visualize RMSEs
bwplot(resamp, metric = "RMSE")
```

Given the output and plot, MARS model has the lowest median RMSE. Hence, I will use this as my final model.

