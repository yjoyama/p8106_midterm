---
title: "Analysis_Chen_Liang_cl4469"
author: "Chen Liang"
date: "2024-03-20"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(mgcv)
library(earth)
library(rsample)
```

## Data preparation
```{r}
# Load data
recov_df <- get(load("./data/recovery.RData")) |> 
  janitor::clean_names() |>
  na.omit()

summary(recov_df)
# Create a partition index.(training:test=80:20)
set.seed(2024)
train_index = initial_split(recov_df, prop = .80)

# Extract the training and test data
training_df = training(train_index) |>select(-id)
testing_df = testing(train_index) |>select(-id)
# Training data
x = model.matrix(recovery_time~.,training_df)[, -1]
y = training_df$recovery_time

# Testing data
x2 <- model.matrix(recovery_time~.,testing_df)[, -1]
y2 <- testing_df$recovery_time

```

## Linear Model
```{r}
set.seed(2024)

# 10-fold cv
ctrl1 <- trainControl(method = "cv", number = 10)

# Fit Model
lm_fit <- train(x, y, method = "lm", trControl = ctrl1)
summary(lm_fit)

# Calculate test error
lm_test_pred <- predict(lm_fit, newdata = x2) 
lm_test_rmse <- sqrt(mean((lm_test_pred - y2)^2))
lm_test_rmse
```

## Lasso Model
```{r}
set.seed(2024)

# Fit Model
lasso_fit <- train(x, y,
                   data= training_df,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(5, -5, length = 100))),
                   trControl = ctrl1)

# Plot RMSE and lambda
plot(lasso_fit, xTrans = log)

# Check best tune
lasso_fit$bestTune

# Obtain coefficients in the final model
coef(lasso_fit$finalModel, s = lasso_fit$bestTune$lambda)

# Calculate test error
lasso_test_pred <- predict(lasso_fit, newdata = x2)
lasso_test_rmse <- mean((lasso_test_pred - y2)^2)
lasso_test_rmse
```

## Elastic Net Model
```{r}
set.seed(2024)

# Fit Model
enet_fit <- train(x, y,
                  data = training_df,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(7, -3, length = 100))),
                  trControl = ctrl1)

# Check best tune
enet_fit$bestTune

# plot RMSE vs lambda and alpha
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet_fit, par.settings = myPar)

# Obtain coefficients in the final model
coef(enet_fit$finalModel, enet_fit$bestTune$lambda)

# Calculate test error
enet_test_pred <- predict(enet_fit, newdata = x2)
enet_test_rmse <- mean((enet_test_pred - y2)^2)
enet_test_rmse
```

## Ridge
```{r}
set.seed(2024)

# Fit Model
ridge_fit <- train(x, y,
                   method = "glmnet", 
                   tuneGrid = expand.grid(alpha = 0,
                                          lambda = exp(seq(8, -1, length=100))),
                   trControl = ctrl1)

# plot RMSE
plot(ridge_fit, xTrans = log)

# Check best tune
ridge_fit$bestTune

# Obtain coefficients in the final model
coef(ridge_fit$finalModel, s = ridge_fit$bestTune$lambda)

# Calculate test error
ridge_test_pred <- predict(ridge_fit, newdata = x2)
ridge_test_mse <- mean((ridge_test_pred - y2)^2)
ridge_test_mse
```

## Principal Component Regression (PCR)
```{r}
set.seed(2024)

# Fit Model
pcr_fit <-  train(x, y,
                  method = "pcr",
                  tuneGrid = data.frame(ncomp = 1:18),
                  trControl = ctrl1,
                  preProcess = c("center", "scale"))

# plot RMSE
ggplot(pcr_fit, highlight = TRUE) + theme_bw()

# Check best tune
pcr_fit$bestTune

# Obtain coefficients in the final model
coef(pcr_fit$finalModel, s = pcr_fit$bestTune)

# Calculate test error
pcr_test_pred <- predict(pcr_fit, newdata = x2)
pcr_test_mse <- mean((pcr_test_pred - y2)^2)
pcr_test_mse
```

## Partial Least Squares model (PLS)
```{r}
set.seed(2024)

# Fit Model
pls_fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:17),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

# Plot RMSE
ggplot(pls_fit, highlight = TRUE)

# Check best tune
pls_fit$bestTune

# Obtain coefficients in the final model
coef(pls_fit$finalModel, s = pls_fit$bestTune)

# Calculate test error
pls_test_pred <- predict(pls_fit, newdata = x2) 
pls_test_mse <- mean((pls_test_pred - y2)^2)
pls_test_mse
```

## GAM
```{r}
set.seed(2024)

# Fit Model
gam_fit = train(x, y,
                method = "gam",   
                tuneGrid = data.frame(method = "GCV.Cp",
                                      select = c(TRUE, FALSE)),
                trControl = ctrl1)

# Parameters that fit the best model
gam_fit$bestTune
gam_fit$finalModel

# View the model summary
summary(gam_fit$finalModel)

# Calculate test error
gam_test_pred <- predict(gam_fit, newdata = x2)
gam_test_mse <- mean((gam_test_pred - y2)^2)
gam_test_mse
```

## Multivariate Adaptive Regression Splines (MARS)
```{r}
set.seed(2024)

mars_grid <- expand.grid(degree = 1:5,
                         nprune = 2:30)


mars_fit <- train(x, y,
                  method = "earth",
                  tuneGrid = mars_grid,
                  metric = "RMSE",
                  trControl = ctrl1)

# Plot RMSE
ggplot(mars_fit, highlight = T)

# Parameters that fit the best model
mars_fit$bestTune

coef(mars_fit$finalModel)

summary(mars_fit$finalModel)

mars_test_pred <- predict(gam_fit, newdata = x2)
mars_test_mse <- mean((mars_test_pred - y2)^2)
mars_test_mse

```

## Model Comparing

```{r}
# resample
resamp <- resamples(list(
  lm = lm_fit,
  lasso = lasso_fit,
  ridge = ridge_fit,
  elastic_net = enet_fit,
  pcr = pcr_fit,
  pls = pls_fit,
  gam = gam_fit,
  mars = mars_fit
))

summary(resamp)

# visualization
bwplot(resamp, metric = "RMSE")
```





